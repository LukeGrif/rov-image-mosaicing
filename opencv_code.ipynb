{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab0787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 59 images.\n",
      "Image 1 - 0: 1991 good matches\n",
      "Image 2 - 1: 1772 good matches\n",
      "Image 3 - 2: 555 good matches\n",
      "Image 4 - 3: 1768 good matches\n",
      "Image 5 - 4: 1836 good matches\n",
      "Image 6 - 5: 1484 good matches\n",
      "Image 7 - 6: 800 good matches\n",
      "Image 8 - 7: 170 good matches\n",
      "Image 9 - 8: 772 good matches\n",
      "Image 10 - 9: 1900 good matches\n",
      "Image 11 - 10: 113 good matches\n",
      "Image 11 - 9: 116 good matches\n",
      "Image 12 - 11: 626 good matches\n",
      "Image 13 - 12: 130 good matches\n",
      "Image 14 - 13: 87 good matches\n",
      "Image 15 - 14: 711 good matches\n",
      "Image 16 - 15: 307 good matches\n",
      "Image 17 - 16: 1861 good matches\n",
      "Image 18 - 17: 481 good matches\n",
      "Image 19 - 18: 1432 good matches\n",
      "Image 20 - 19: 648 good matches\n",
      "Image 21 - 20: 144 good matches\n",
      "Image 22 - 21: 1562 good matches\n",
      "Image 23 - 22: 1795 good matches\n",
      "Image 24 - 23: 1669 good matches\n",
      "Image 25 - 24: 1317 good matches\n",
      "Image 26 - 25: 497 good matches\n",
      "Image 27 - 26: 2799 good matches\n",
      "Image 28 - 27: 2368 good matches\n",
      "Image 29 - 28: 196 good matches\n",
      "Image 30 - 29: 612 good matches\n",
      "Image 31 - 30: 583 good matches\n",
      "Image 32 - 31: 953 good matches\n",
      "Image 33 - 32: 1468 good matches\n",
      "Image 34 - 33: 376 good matches\n",
      "Image 35 - 34: 1147 good matches\n",
      "Image 36 - 35: 234 good matches\n",
      "Image 37 - 36: 459 good matches\n",
      "Image 38 - 37: 103 good matches\n",
      "Image 38 - 36: 88 good matches\n",
      "Image 38 - 35: 111 good matches\n",
      "Image 39 - 38: 124 good matches\n",
      "Image 39 - 37: 111 good matches\n",
      "Image 39 - 36: 98 good matches\n",
      "Image 39 - 35: 112 good matches\n",
      "Image 39 - 34: 114 good matches\n",
      "Image 40 - 39: 199 good matches\n",
      "Image 41 - 40: 309 good matches\n",
      "Image 42 - 41: 225 good matches\n",
      "Image 43 - 42: 603 good matches\n",
      "Image 44 - 43: 331 good matches\n",
      "Image 45 - 44: 179 good matches\n",
      "Image 46 - 45: 89 good matches\n",
      "Image 46 - 44: 81 good matches\n",
      "Image 46 - 43: 85 good matches\n",
      "Image 46 - 42: 96 good matches\n",
      "Image 46 - 41: 76 good matches\n",
      "Image 46 - 40: 93 good matches\n",
      "Image 46 - 39: 86 good matches\n",
      "Image 46 - 38: 72 good matches\n",
      "Image 46 - 37: 95 good matches\n",
      "Image 47 - 46: 145 good matches\n",
      "Image 48 - 47: 88 good matches\n",
      "Image 49 - 48: 88 good matches\n",
      "Image 50 - 49: 167 good matches\n",
      "Image 50 - 48: 121 good matches\n",
      "Image 50 - 47: 147 good matches\n",
      "Image 50 - 46: 142 good matches\n",
      "Image 50 - 45: 133 good matches\n",
      "Image 50 - 44: 144 good matches\n",
      "Image 51 - 50: 186 good matches\n",
      "Image 52 - 51: 259 good matches\n",
      "Image 53 - 52: 224 good matches\n",
      "Image 54 - 53: 213 good matches\n",
      "Image 55 - 54: 237 good matches\n",
      "Image 56 - 55: 106 good matches\n",
      "Image 56 - 54: 116 good matches\n",
      "Image 57 - 56: 144 good matches\n",
      "Image 58 - 57: 157 good matches\n",
      "Mosaic canvas (full-res coords): 27800 x 40787\n",
      "Rendering output canvas: 9730 x 14275 (scale=0.35)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'BORDER_NEAREST'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 244\u001b[39m\n\u001b[32m    238\u001b[39m H_off = (S @ T @ H).astype(np.float32)\n\u001b[32m    239\u001b[39m warped = cv2.warpPerspective(images[idx], H_off, (W_out, H_out),\n\u001b[32m    240\u001b[39m                              flags=cv2.INTER_LINEAR,\n\u001b[32m    241\u001b[39m                              borderMode=cv2.BORDER_CONSTANT, borderValue=\u001b[32m0\u001b[39m)\n\u001b[32m    242\u001b[39m mask = cv2.warpPerspective(\n\u001b[32m    243\u001b[39m     np.ones(sizes[idx], np.float32), H_off, (W_out, H_out),\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     flags=\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBORDER_NEAREST\u001b[49m, borderValue=\u001b[32m0\u001b[39m\n\u001b[32m    245\u001b[39m )\n\u001b[32m    246\u001b[39m accum += warped.astype(np.float32) * mask[..., \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    247\u001b[39m weight += mask\n",
      "\u001b[31mAttributeError\u001b[39m: module 'cv2' has no attribute 'BORDER_NEAREST'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Config (tune as needed)\n",
    "# =========================\n",
    "image_glob = \"Killaloe/*.jpg\"   # <-- change if needed\n",
    "RATIO = 0.80                    # Lowe ratio (looser to recover weak matches)\n",
    "RANSAC_REPROJ_THRESH = 4.0      # RANSAC reprojection threshold (px)\n",
    "MIN_MATCH_COUNT = 8             # minimum \"good\" matches to attempt placement\n",
    "BACK_WINDOW = 20                # how many previous frames to try as anchors\n",
    "MAX_FEATURES = 8000             # SIFT nfeatures\n",
    "DOWNSCALE_FOR_FEATURES = 1.0    # 1.0 = off; try 0.75 for speed once stable\n",
    "\n",
    "# Prevent gigantic raster output (render smaller while aligning at full-res)\n",
    "OUTPUT_DOWNSCALE = 0.35         # 1.0 = full-size output; 0.25â€“0.5 is common\n",
    "\n",
    "# Transform sanity thresholds (per-step)\n",
    "SCALE_MIN, SCALE_MAX = 0.7, 1.3\n",
    "MAX_SHEAR_COS = 0.2             # |cos(angle between basis)|, 0 = orthogonal\n",
    "MAX_SHIFT_FACTOR = 1.2          # max per-step shift as a fraction of image diagonal\n",
    "\n",
    "# Transform cache file\n",
    "TRANSFORM_FILE = \"mosaic_transforms.npz\"\n",
    "USE_SAVED_TRANSFORMS = True     # If True and file exists, skip matching and just render\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def load_images(pattern):\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    if not files:\n",
    "        raise IOError(\"No images found. Check the glob/path.\")\n",
    "    imgs = [cv2.imread(f) for f in files]\n",
    "    ok = [(f, im) for f, im in zip(files, imgs) if im is not None]\n",
    "    if not ok:\n",
    "        raise IOError(\"Images could not be read. Check formats/paths.\")\n",
    "    files = [f for f, _ in ok]\n",
    "    imgs  = [im for _, im in ok]\n",
    "    print(f\"Loaded {len(imgs)} images.\")\n",
    "    return files, imgs\n",
    "\n",
    "def small(im):\n",
    "    if DOWNSCALE_FOR_FEATURES == 1.0:\n",
    "        return im\n",
    "    h, w = im.shape[:2]\n",
    "    return cv2.resize(im, (int(w*DOWNSCALE_FOR_FEATURES), int(h*DOWNSCALE_FOR_FEATURES)))\n",
    "\n",
    "# CLAHE + light denoise helps underwater contrast\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "def prep_gray_for_features(img):\n",
    "    g = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    g = clahe.apply(g)\n",
    "    g = cv2.GaussianBlur(g, (3,3), 0)\n",
    "    return g\n",
    "\n",
    "def scale_pts(pts):\n",
    "    \"\"\"Map keypoint coords from downscaled coords back to full-res.\"\"\"\n",
    "    s = 1.0 / DOWNSCALE_FOR_FEATURES\n",
    "    return pts * s\n",
    "\n",
    "def transform_ok(H_2x3, img_shape):\n",
    "    \"\"\"\n",
    "    Check similarity/affine for sane scale, shear, and shift magnitude.\n",
    "    H_2x3: 2x3 affine (from estimateAffinePartial2D/2D)\n",
    "    \"\"\"\n",
    "    h, w = img_shape[:2]\n",
    "    A = H_2x3[:, :2]\n",
    "    t = H_2x3[:, 2]\n",
    "    # scales\n",
    "    s1 = np.linalg.norm(A[:, 0]) + 1e-12\n",
    "    s2 = np.linalg.norm(A[:, 1]) + 1e-12\n",
    "    scale = 0.5*(s1+s2)\n",
    "    if not (SCALE_MIN <= scale <= SCALE_MAX):\n",
    "        return False\n",
    "    # shear (basis orthogonality)\n",
    "    cosang = abs(np.dot(A[:,0]/s1, A[:,1]/s2))\n",
    "    if cosang > MAX_SHEAR_COS:\n",
    "        return False\n",
    "    # shift\n",
    "    diag = np.hypot(h, w)\n",
    "    if np.linalg.norm(t) > MAX_SHIFT_FACTOR*diag:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def to_homography_from_affine(A2x3):\n",
    "    H = np.eye(3, dtype=np.float32)\n",
    "    H[:2, :] = A2x3.astype(np.float32)\n",
    "    return H\n",
    "\n",
    "# =========================\n",
    "# Compute transforms (or load cache)\n",
    "# =========================\n",
    "if USE_SAVED_TRANSFORMS and os.path.exists(TRANSFORM_FILE):\n",
    "    data = np.load(TRANSFORM_FILE, allow_pickle=True)\n",
    "    H_to_base = list(data[\"H_to_base\"])\n",
    "    sizes = list(data[\"sizes\"])\n",
    "    files = list(data[\"files\"])\n",
    "    T = data[\"T\"]\n",
    "    W = int(data[\"W\"])\n",
    "    Hh = int(data[\"Hh\"])\n",
    "    print(f\"Loaded transforms from {TRANSFORM_FILE}\")\n",
    "else:\n",
    "    # Load images once for feature extraction\n",
    "    files, images = load_images(image_glob)\n",
    "\n",
    "    # SIFT + FLANN\n",
    "    try:\n",
    "        sift = cv2.SIFT_create(nfeatures=MAX_FEATURES)\n",
    "    except AttributeError:\n",
    "        sift = cv2.xfeatures2d.SIFT_create(nfeatures=MAX_FEATURES)\n",
    "    flann = cv2.FlannBasedMatcher(dict(algorithm=1, trees=5), dict(checks=64))\n",
    "\n",
    "    # Detect & describe (on downscaled/preprocessed)\n",
    "    kps, descs, sizes = [], [], []\n",
    "    for im in images:\n",
    "        im_small = small(im)\n",
    "        gs = prep_gray_for_features(im_small)\n",
    "        kp, des = sift.detectAndCompute(gs, None)\n",
    "        kps.append(kp)\n",
    "        descs.append(des)\n",
    "        sizes.append(im.shape[:2])  # full-res size\n",
    "\n",
    "    def match_des(i, j):\n",
    "        if descs[i] is None or descs[j] is None:\n",
    "            return []\n",
    "        matches = flann.knnMatch(descs[i], descs[j], k=2)\n",
    "        good = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < RATIO * n.distance:\n",
    "                good.append(m)\n",
    "        return good\n",
    "\n",
    "    def find_H_to_anchor(i, j):\n",
    "        \"\"\"\n",
    "        Find transform mapping image i -> image j (full-res coords).\n",
    "        Prefer similarity (estimateAffinePartial2D) to avoid projective drift.\n",
    "        Fallback to full affine; use homography only as a last resort.\n",
    "        \"\"\"\n",
    "        good = match_des(i, j)\n",
    "        print(f\"Image {i} - {j}: {len(good)} good matches\")\n",
    "        if len(good) < 4:\n",
    "            return None\n",
    "\n",
    "        src = np.float32([kps[i][m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
    "        dst = np.float32([kps[j][m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
    "        src = scale_pts(src); dst = scale_pts(dst)\n",
    "\n",
    "        # 1) Similarity (rotation+translation+uniform scale)\n",
    "        A_sim, inl = cv2.estimateAffinePartial2D(src, dst, method=cv2.RANSAC,\n",
    "                                                 ransacReprojThreshold=RANSAC_REPROJ_THRESH)\n",
    "        if A_sim is not None and transform_ok(A_sim, sizes[i]):\n",
    "            return to_homography_from_affine(A_sim)\n",
    "\n",
    "        # 2) Full affine\n",
    "        A_aff, inl2 = cv2.estimateAffine2D(src, dst, method=cv2.RANSAC,\n",
    "                                           ransacReprojThreshold=RANSAC_REPROJ_THRESH)\n",
    "        if A_aff is not None and transform_ok(A_aff, sizes[i]):\n",
    "            return to_homography_from_affine(A_aff)\n",
    "\n",
    "        # 3) Homography as last resort (but clamp projective terms)\n",
    "        H, mask = cv2.findHomography(src, dst, cv2.RANSAC, RANSAC_REPROJ_THRESH)\n",
    "        if H is not None and H.shape == (3,3):\n",
    "            # Reject strong projective components that cause canvas blow-up\n",
    "            if abs(H[2,0]) < 1e-4 and abs(H[2,1]) < 1e-4:\n",
    "                A = H[:2,:2]\n",
    "                s1 = np.linalg.norm(A[:,0]) + 1e-12\n",
    "                s2 = np.linalg.norm(A[:,1]) + 1e-12\n",
    "                scale = 0.5*(s1+s2)\n",
    "                if SCALE_MIN <= scale <= SCALE_MAX:\n",
    "                    return H.astype(np.float32)\n",
    "        return None\n",
    "\n",
    "    # Build accumulated transforms (to base 0)\n",
    "    H_to_base = [np.eye(3, dtype=np.float32)]\n",
    "    placed = [0]\n",
    "    for i in range(1, len(images)):\n",
    "        H_acc = None\n",
    "        # dynamic expansion: try BACK_WINDOW, then 2*BACK_WINDOW\n",
    "        for win in (BACK_WINDOW, 2*BACK_WINDOW):\n",
    "            lookbacks = range(1, min(win, i) + 1)\n",
    "            for k in lookbacks:\n",
    "                j = i - k\n",
    "                if H_to_base[j] is None:\n",
    "                    continue\n",
    "                H_ij = find_H_to_anchor(i, j)\n",
    "                if H_ij is not None:\n",
    "                    H_acc = (H_to_base[j] @ H_ij).astype(np.float32)\n",
    "                    break\n",
    "            if H_acc is not None:\n",
    "                break\n",
    "\n",
    "        if H_acc is None:\n",
    "            print(f\"Unable to place image {i} (no robust anchor found up to {2*BACK_WINDOW}). Skipping.\")\n",
    "            H_to_base.append(None)\n",
    "        else:\n",
    "            H_to_base.append(H_acc)\n",
    "            placed.append(i)\n",
    "\n",
    "    if not any(H is not None for H in H_to_base):\n",
    "        raise RuntimeError(\"No images could be placed. Consider loosening thresholds or improving contrast.\")\n",
    "\n",
    "    # Compute mosaic bounds\n",
    "    corners = []\n",
    "    for idx, H in enumerate(H_to_base):\n",
    "        if H is None:\n",
    "            continue\n",
    "        h, w = sizes[idx]\n",
    "        pts = np.float32([[0,0],[w,0],[w,h],[0,h]]).reshape(-1,1,2)\n",
    "        warped = cv2.perspectiveTransform(pts, H)\n",
    "        corners.append(warped)\n",
    "\n",
    "    all_c = np.vstack(corners)\n",
    "    x_min, y_min = all_c.min(axis=0).ravel()\n",
    "    x_max, y_max = all_c.max(axis=0).ravel()\n",
    "\n",
    "    W = int(np.ceil(x_max - x_min))\n",
    "    Hh = int(np.ceil(y_max - y_min))\n",
    "    tx = -int(np.floor(x_min)) if x_min < 0 else 0\n",
    "    ty = -int(np.floor(y_min)) if y_min < 0 else 0\n",
    "    T = np.array([[1,0,tx],[0,1,ty],[0,0,1]], dtype=np.float32)\n",
    "\n",
    "    print(f\"Mosaic canvas (full-res coords): {W} x {Hh}\")\n",
    "\n",
    "    # Save transforms for fast re-render later\n",
    "    np.savez(TRANSFORM_FILE,\n",
    "             H_to_base=np.array(H_to_base, dtype=object),\n",
    "             sizes=np.array(sizes, dtype=object),\n",
    "             files=np.array(files, dtype=object),\n",
    "             T=T, W=W, Hh=Hh)\n",
    "    print(f\"Saved transforms to {TRANSFORM_FILE}\")\n",
    "\n",
    "# =========================\n",
    "# Render (uses saved or freshly-computed transforms)\n",
    "# =========================\n",
    "S = np.array([[OUTPUT_DOWNSCALE, 0, 0],\n",
    "              [0, OUTPUT_DOWNSCALE, 0],\n",
    "              [0, 0, 1]], dtype=np.float32)\n",
    "W_out = max(1, int(W * OUTPUT_DOWNSCALE))\n",
    "H_out = max(1, int(Hh * OUTPUT_DOWNSCALE))\n",
    "print(f\"Rendering output canvas: {W_out} x {H_out} (scale={OUTPUT_DOWNSCALE})\")\n",
    "\n",
    "accum = np.zeros((H_out, W_out, 3), np.float32)\n",
    "weight = np.zeros((H_out, W_out), np.float32)\n",
    "\n",
    "for idx, H in enumerate(H_to_base):\n",
    "    if H is None:\n",
    "        continue\n",
    "    # Load on demand to keep memory modest in render step\n",
    "    img = cv2.imread(files[idx])\n",
    "    if img is None:\n",
    "        print(f\"Warning: could not read {files[idx]} at render time; skipping.\")\n",
    "        continue\n",
    "    H_off = (S @ T @ H).astype(np.float32)\n",
    "\n",
    "    warped = cv2.warpPerspective(img, H_off, (W_out, H_out),\n",
    "                                 flags=cv2.INTER_LINEAR,\n",
    "                                 borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "\n",
    "    # FIXED: INTER_NEAREST (not BORDER_NEAREST)\n",
    "    mask = cv2.warpPerspective(\n",
    "        np.ones((sizes[idx][0], sizes[idx][1]), np.float32), H_off, (W_out, H_out),\n",
    "        flags=cv2.INTER_NEAREST, borderValue=0\n",
    "    )\n",
    "\n",
    "    accum += warped.astype(np.float32) * mask[..., None]\n",
    "    weight += mask\n",
    "\n",
    "weight[weight == 0] = 1e-6\n",
    "mosaic = (accum / weight[..., None]).clip(0,255).astype(np.uint8)\n",
    "\n",
    "out_name = \"stitched_mosaic.jpg\"\n",
    "cv2.imwrite(out_name, mosaic)\n",
    "print(f\"Saved {out_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
